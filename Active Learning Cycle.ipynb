{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](figure_1_burr_settles.png)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fig 1 Credit: Burr Settles. 2009. “Active Learning Literature Survey.” Computer Sciences Technical Report 1648. University of Wisconsin–Madison.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook demonstrates the process of Active Learning (AL) using features from the Stanford movies review dataset (http://ai.stanford.edu/~amaas/data/sentiment/) following the CRISP-DM process (https://paginas.fe.up.pt/~ec/files_0405/slides/02%20CRISP.pdf).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statement of Business Objective**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The business would like to gain more value from unstructured text data while optimizing use of labor resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statement of Machine Learning Objective**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine learning system should enable the business to indepententy generate concepts and perform classification on their own data.  The system will use AL to deliver increased value from unstructured data.\n",
    "\n",
    "AL accomplishes this through an iterative process that trains on the labeled features and then predicts on the unlabeled samples' features.  These predictions can then be evaluated in a manner to suggest the next desirable review for the expert to label.  \n",
    "\n",
    "For example, ordering based on distance from the classification boundary allows prioritization of the sample that may be questionable.  Once labeled, the iterative process repeats, the labeled features are used to train the model, and predictions are evaluated to provide the next sample(s) for labeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statement of Success Criteria**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AL is a process that relies on an expert's feedback to label samples and drive the machine learning process.  The benefit of Active Learning with Rationales is to learn a model of the concept (in this case sentiment) in a manner that further minimizes the need for the expert's time (resources).  Thus this effort will show the advantage of using rationales to reduce the need for business resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Understanding** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of sentiment (as the concept), the positive and negative labels simply a boolean value for each sample.  Here is a positive sentiment sample from the movie review dataset:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[train/pos/1999_9.txt]**   \n",
    "*Oliver! the musical is a favorite of mine. The music, the characters, the story. It all just seems perfect. In this rendition of the timeless classic novel turned stage musical, director Carol Reed brings the Broadway hit to life on the movie screen.<br /><br />The transition from musical to movie musical is not an easy one. You have to have the right voices, the right set, the right script, and the right play. All signs point to yes for this play. It almost appears that it was written for the screen!<br /><br />Our story takes place in jolly old England where a boy named Oliver manages to work his way out of the orphanage. He winds his way through the country to London where he meets up with a group of juvenile delinquents, headed by Dodger, the smart talking, quick handed pick-pocket. The leader of this gang is named Fagin, an older fellow who sells all the stolen goods.<br /><br />But all is not well in London town when Bill Sykes played by Oliver Reed and his loving girlfriend Nancy get tangled up with Oliver, Fagin and his young troops, and the law. What ensues is a marvelous tale of love, affection, and great musical numbers.<br /><br />Whether or not you like musicals or not, one listen to these tunes and you will be humming them all day long. Oliver! is a triumph on and off the stage and is a timeless work of art.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The movie review data dataset does not have rationales.  Amazon's Mechanical Turk provides a way to gather the rationales for both the positive and negative reviews.  The first acceptable rationale for each review is selected for a balanced number of postive and negative samples.  A pretrained word2vec model is used to generate word level features embeddings.  Then these results are saved for the next stage since they are used many times (as seen later for grid search and shuffle processing).\n",
    "\n",
    "Here is the source code for filtering the input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import pandas as pd\n",
      "import random\n",
      "import numpy as np\n",
      "import pickle\n",
      "\n",
      "# load data\n",
      "pos_df = pd.read_csv('../../../Downloads/Batch_3494319_batch_results.csv')\n",
      "neg_df = pd.read_csv('../../../Downloads/Batch_3494476_batch_results.csv')\n",
      "\n",
      "# only get the accepted records from the mechanical turk results\n",
      "pos_df_approved = pos_df[pos_df['AssignmentStatus']=='Approved']\n",
      "neg_df_approved = neg_df[neg_df['AssignmentStatus']=='Approved']\n",
      "\n",
      "# get a balanced set of highlighted postive and negative reviews\n",
      "pos_df_approved = pos_df_approved.drop_duplicates(subset='Input.text', keep='first')\n",
      "neg_df_approved = neg_df_approved.drop_duplicates(subset='Input.text', keep='first')\n",
      "balanced_count = min(len(pos_df_approved), len(neg_df_approved))\n",
      "\n",
      "# buid the dataset to contain the \n",
      "labeled_pos = random.sample(pos_df_approved['Answer.highlights'].tolist(), balanced_count)\n",
      "labeled_neg = random.sample(neg_df_approved['Answer.highlights'].tolist(), balanced_count)\n",
      "\n",
      "# here, negative is mapped to zere and positive to one\n",
      "X = (labeled_neg + labeled_pos)\n",
      "Y = np.concatenate([np.zeros(balanced_count),np.ones(balanced_count)])\n",
      "\n",
      "# lets save these for the next stage in this pipeline\n",
      "with open('X.pkl', 'wb') as f:\n",
      "    pickle.dump(X, f)\n",
      "with open('Y.pkl', 'wb') as f:\n",
      "    pickle.dump(Y, f)\n"
     ]
    }
   ],
   "source": [
    "!cat process_step_1_read_split_save.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the source code for finding and saving the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pickle\n",
      "import numpy as np\n",
      "import pymagnitude\n",
      "\n",
      "# load input data\n",
      "X = pickle.load( open('X.pkl', 'rb') )\n",
      "\n",
      "# load the pretrained word2vec model for feature assignment\n",
      "pretrained_magnitude = r'../../../Downloads/pretrained/glove.6B.300d.magnitude'\n",
      "vectors = pymagnitude.Magnitude(pretrained_magnitude)\n",
      "\n",
      "# setup speciality cleaning\n",
      "def get_document_features(data_in):\n",
      "    \"\"\"Used to clean 80k Mechanical Turk responses.\n",
      "\n",
      "    Params:\n",
      "        data_in --  text segment to process\n",
      "    Returns:\n",
      "        features for input text and features\n",
      "    \"\"\"\n",
      "    data_in = data_in.replace('<span class=\\\"active_text\\\">', '').replace('</span>', '')\n",
      "    body = data_in.split(r'\\n                                    ')[1].replace('\\n', '')\n",
      "    avg_vec = np.mean(vectors.query(body.split(' ')), axis=(0))\n",
      "    \n",
      "    if highlights == True:\n",
      "        high_text = data_in.split(r'\\n                                    ')[0].replace('\\n', '')\n",
      "        high_avg_vec = np.mean(vectors.query(high_text.split(' ')), axis=(0))\n",
      "        return avg_vec,  high_avg_vec\n",
      "    else:\n",
      "        return avg_vec\n",
      "        \n",
      "# preprocess the training set features\n",
      "X_reg_features = list()\n",
      "X_high_features = list()\n",
      "for idx,text in enumerate(X):\n",
      "    avg_vec, high_avg_vec = get_document_features(text)\n",
      "    X_reg_features.append(avg_vec)\n",
      "    X_high_features.append(high_avg_vec)\n",
      "    print(\"processing \" + str(idx/len(X))) \n",
      "\n",
      "# we save these precomputed results since they are used repeatedly downstream\n",
      "with open('X_reg_features.pkl', 'wb') as f:\n",
      "     pickle.dump(X_reg_features, f)\n",
      "with open('X_high_features.pkl', 'wb') as f:\n",
      "     pickle.dump(X_high_features, f)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat process_step_2_assign_features.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to first find "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# open the upstream data\n",
      "import pickle\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score, roc_curve, auc, roc_auc_score\n",
      "import numpy as np\n",
      "\n",
      "def find_nearest(array, value):\n",
      "    \"\"\"Finds the prediction closest to the boundary\"\"\"\n",
      "    array = np.asarray(array)\n",
      "    return (np.abs(array - value)).argmin()\n",
      "\n",
      "# different values of c (.1 increments)\n",
      "for c in [0.5,0.2,0.8,0.4,0.6,0.7,0.3,0.9,0.1,0.0,1.0]:\n",
      "    # import the feature data and labels\n",
      "    Y = pickle.load(  open('Y.pkl', \"rb\" ) )\n",
      "    X_reg_features = pickle.load(  open('X_reg_features.pkl', \"rb\" ) )\n",
      "    X_high_features = pickle.load(  open('X_high_features.pkl', \"rb\" ) )\n",
      "\n",
      "    # apply c to show how different proportions impact AL performance\n",
      "    for idx,f in enumerate(X_high_features):\n",
      "        X_high_features[idx] =  (X_high_features[idx] * c) + (1-c) * f\n",
      "\n",
      "    # reshuffle 20 times and take the average\n",
      "    # for episode in range(1,20):\n",
      "    episode = 1\n",
      "\n",
      "    # split the dataset\n",
      "    X_train_reg, X_test_reg, Y_train_reg, Y_test_reg = train_test_split(X_reg_features, Y, test_size=0.2)\n",
      "    X_train_high, X_test_high, Y_train_high, Y_test_high = train_test_split(X_high_features, Y, test_size=0.2)\n",
      "\n",
      "    # used to accumulate training text samples for simulating active learning\n",
      "    y_train_list_baseline = list()\n",
      "    y_train_list_highlights = list()\n",
      "\n",
      "    # used to accumulate training sample features\n",
      "    norm_feat_train_list = list()\n",
      "    high_feat_train_list = list()\n",
      "\n",
      "    # used to capture auc for baseline and comparison\n",
      "    norm_auc = list()\n",
      "    high_auc = list()\n",
      "\n",
      "    # seed the initial model\n",
      "    baseline_queue_index = np.nonzero(Y_train_reg == 0)[0][0]\n",
      "    highlight_queue_index = np.nonzero(Y_train_high == 0)[0][0]\n",
      "\n",
      "    norm_feat_train_list.append(X_train_reg[baseline_queue_index])\n",
      "    X_train_reg.pop(baseline_queue_index)\n",
      "    high_feat_train_list.append(X_train_high[highlight_queue_index])\n",
      "    X_train_high.pop(highlight_queue_index)\n",
      "\n",
      "    y_train_list_baseline.append(Y_train_reg[baseline_queue_index])\n",
      "    Y_train_reg = np.delete(Y_train_reg, Y_train_reg[baseline_queue_index])\n",
      "\n",
      "    y_train_list_highlights.append(Y_train_high[highlight_queue_index])\n",
      "    Y_train_high = np.delete(Y_train_high,Y_train_high[highlight_queue_index])\n",
      "\n",
      "    # let the first iteration \n",
      "    baseline_queue_index = np.nonzero(Y_train_reg == 1)[0][0]\n",
      "    highlight_queue_index = np.nonzero(Y_train_high == 1)[0][0]\n",
      "\n",
      "    # simulated active learning\n",
      "    for idx in range(0,len(Y_train_reg)):\n",
      "        # We add it to the simulated processed data, then remove it from the available training sample queue\n",
      "        norm_feat_train_list.append(X_train_reg[baseline_queue_index])\n",
      "        X_train_reg.pop(baseline_queue_index)\n",
      "        high_feat_train_list.append(X_train_high[highlight_queue_index])\n",
      "        X_train_high.pop(highlight_queue_index)\n",
      "\n",
      "        # same applies for the y\n",
      "        y_train_list_baseline.append(Y_train_reg[baseline_queue_index])\n",
      "        Y_train_reg = np.delete(Y_train_reg, Y_train_reg[baseline_queue_index])\n",
      "        y_train_list_highlights.append(Y_train_high[highlight_queue_index])\n",
      "        Y_train_high = np.delete(Y_train_high,Y_train_high[highlight_queue_index])\n",
      "\n",
      "        # Train each model\n",
      "        reg_model = LogisticRegression().fit(np.vstack(norm_feat_train_list), y_train_list_baseline)\n",
      "        high_model = LogisticRegression().fit(np.vstack(high_feat_train_list), y_train_list_highlights)\n",
      "\n",
      "        # find AUC for baseline\n",
      "        reg_probas_ = reg_model.decision_function(X_test_reg)\n",
      "        false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test_reg, reg_probas_)\n",
      "        n_auc = auc(false_positive_rate, true_positive_rate)\n",
      "        norm_auc.append( n_auc )\n",
      "        \n",
      "        # find AUC for highlights\n",
      "        high_probas_ = high_model.decision_function(X_test_high)\n",
      "        false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test_high, high_probas_)\n",
      "        h_auc = auc(false_positive_rate, true_positive_rate)\n",
      "        high_auc.append( h_auc )\n",
      "\n",
      "        # select most 'unknown' for baseline, but not prediction is needed if empty\n",
      "        if len(X_train_reg) > 0:\n",
      "            # find the index of the next most valuable sample to label\n",
      "            predictions = reg_model.predict(X_train_reg)\n",
      "            baseline_queue_index = find_nearest(predictions,0.5)\n",
      "        \n",
      "        # select most 'unknown' for rationales, but not prediction is needed if empty\n",
      "        if len(X_train_high) > 0:\n",
      "            predictions = high_model.predict(X_train_high)\n",
      "            highlight_queue_index = find_nearest(predictions,0.5)\n",
      "\n",
      "        print('c = ' + str(c) + ' processed ' + str(idx) + ' remaining ' + str(len(Y_train_reg)))\n",
      "\n",
      "    with open('norm_auc_'+str(c)+'_'+str(episode)+'.pkl', 'wb') as f:\n",
      "        pickle.dump(norm_auc, f)\n",
      "    with open('high_auc_'+str(c)+'_'+str(episode)+'.pkl', 'wb') as f:\n",
      "        pickle.dump(high_auc, f)\n"
     ]
    }
   ],
   "source": [
    "!cat process_step_3_train_score_auc.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deployment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2 Technical Demonstration**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100000\n"
     ]
    }
   ],
   "source": [
    "# get information about the features, these are precomputed and cached locally\n",
    "!ls /Users/username/Downloads/features/ | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we have a total of 100,000 movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get information about a review's features\n",
    "import numpy as np\n",
    "dat = np.fromfile('/Users/username/Downloads/features/train_pos_1999_9.npy',dtype=np.float32) \n",
    "print(len(dat))\n",
    "print(str(dat))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the 300 features listed above are float values.  These particular features represent the positive movie review for *Oliver!*. \n",
    "\n",
    "Lets cover how these 300 features are generated.  Each word in the document is used to find its **word2vec** feature, and then these features are averaged to produce the above document-level feature.  Services like **pymagnitude** are used to create the 300 floats for each word using the query method.  Then, for each document, these are averaged. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a technical understanding of how the document's feature vector is made, let's move forward to see how the iterative Active Learning Service could be implemented using Scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what do you think?  Positive or negative?  Is it apparent why the model may have a harder time classifying this review based on the author's opinion of Justin Timberlake's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Active Learning process repeats by retraining on a slightly larger labeled set of training data.  The updated model predicts on the somewhat smaller collection of unlabeled data so that the service can return the next best unlabeled review..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
